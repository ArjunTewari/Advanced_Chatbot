{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* Chatbots are really helpful to make our lives easy. This chatbot saves two types of memory, a basic on-going conversation and then the summary of the entire conversation as permanent memory in the storage file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problems\n",
    "* Langchain documentations are very confusing, We have to make the chatbot give contexual response and then remember it. Also not becoming so memory expensive by saving entire responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process we will follow\n",
    "1. Create two memories, Temproray and permanent.\n",
    "2. Create a function that stores user and assistant responses in the temprorary and permanent memory.\n",
    "3. Then retrieves it on the basis of the specific user ID.\n",
    "4. Respond to the current question with context to temprorary as well as permanent summary.\n",
    "5. Update the vectorstore or the database with the new conversation as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the Men2.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use converation buffer memory to initialize conversation memory to save on-going conversation to answer contextually. And a summary memory which will be also be saving the responses in a summarized manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "summary_memory = ConversationBufferMemory(llm = llm, max_token_limit = 500)\n",
    "convo_memory = ConversationBufferMemory(llm = llm, max_token_limit = 500)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "def load_faiss_index(file_path=\"faiss_index\"):\n",
    "    return FAISS.load_local(file_path, embeddings, allow_dangerous_deserialization=True)\n",
    "# Add an instruction to guide the assistant\n",
    "system_instruction = \"You are a nice, helpful assistant\"\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"\"], embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Index Inspection and User Summary Retrieval\n",
    "This Python code consists of two functions designed to interact with a FAISS-based vector store. The FAISS index is used for efficient similarity searches in vectorized data, such as text embeddings.\n",
    "\n",
    "1. **inspect_faiss_index(vectorstore)**\n",
    "Purpose:\n",
    "This function inspects the content stored in a FAISS index, including its associated metadata.\n",
    "\n",
    "How It Works:\n",
    "It accesses the docstore dictionary of the vectorstore to retrieve stored texts and their metadata.\n",
    "It iterates through each item in the dictionary and prints:\n",
    "ID: The unique identifier for the stored text.\n",
    "Text: The actual page content stored in the FAISS index.\n",
    "Metadata: Additional information associated with the stored text, such as user-specific data or type.\n",
    "\n",
    "2. **retrieve_user_summary(vectorstore, user_id)**\n",
    "Purpose:\n",
    "This function retrieves the latest summary for a specific user based on their user_id.\n",
    "\n",
    "How It Works:\n",
    "It uses the similarity_search() method of the vectorstore to search for a record that matches:\n",
    "user_id (the specific user identifier).\n",
    "type set to \"summary\" (indicating that the data is a user summary).\n",
    "It retrieves only one result (k=1) and returns the page_content (text) of the result if found.\n",
    "If no matching record is found, it returns an empty string (\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inspect_faiss_index(vectorstore):\n",
    "    \"\"\"\n",
    "    Inspect the texts and metadata stored in the FAISS index.\n",
    "    \"\"\"\n",
    "    # Retrieve stored texts and their metadata\n",
    "    stored_texts = vectorstore.docstore._dict  # Access stored texts\n",
    "    for key, value in stored_texts.items():\n",
    "        print(f\"ID: {key}\")\n",
    "        print(f\"Text: {value.page_content}\")\n",
    "        print(f\"Metadata: {value.metadata}\")\n",
    "        print(\"-\" * 50)\n",
    "def retrieve_user_summary(vectorstore, user_id):\n",
    "    \"\"\"Retrieve the latest summary for a specific user.\"\"\"\n",
    "    results = vectorstore.similarity_search(\"\", k=1, filter={\"user_id\": user_id, \"type\": \"summary\"})\n",
    "    return results[0].page_content if results else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Integration and Conversation Management Code Explanation:\n",
    "\n",
    "This code provides functionality to manage and store conversations using FAISS (Facebook AI Similarity Search). It also includes methods for generating summaries, interacting with an LLM (Language Model), and managing the FAISS index itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Storing Final Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates and stores a final summary of a conversation into the FAISS index. Here's the process:\n",
    "\n",
    "Conversation Retrieval: The conversation history is retrieved using summary_memory.load_memory_variables({}), where the history key contains the entire conversation.\n",
    "\n",
    "Summary Creation: An LLM is instructed to summarize the conversation using a defined summary_instruction and a summary_prompt containing the conversation history.\n",
    "\n",
    "FAISS Storage: The generated summary (final_summary) is stored in the FAISS index, along with metadata specifying the user_id and data type (\"summary\").\n",
    "\n",
    "Output: The function prints confirmation that the summary has been stored successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_instruction = \"You are a concise assistant. Generate a brief summary of the conversation, and who gives contexual answers using the summary stored and the previous recent messages\"\n",
    "def store_final_summary(vectorstore, user_id):\n",
    "    \"\"\"Summarize the entire conversation in memory and store it.\"\"\"\n",
    "    # Retrieve the entire conversation from memory\n",
    "    conversation_history = summary_memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "    # Use LLM to summarize the conversation\n",
    "    summary_prompt = f\"{summary_instruction}\\n\\nConversation History:\\n{conversation_history}\\n\\nSummary:\"\n",
    "    final_summary = llm.invoke(summary_prompt).content.strip()\n",
    "\n",
    "    # Store the summary in FAISS\n",
    "    vectorstore.add_texts(\n",
    "        [final_summary],\n",
    "        metadatas=[{\"user_id\": user_id, \"type\": \"summary\"}]\n",
    "    )\n",
    "    print(\"\\nConversation summarized and stored in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization and Response Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function manages user interactions while summarizing and maintaining a conversation flow:\n",
    "\n",
    "Summary Retrieval: It retrieves an existing summary for the user by invoking the retrieve_user_summary function.\n",
    "\n",
    "User Input Management: The current user input is added to the chat_memory.\n",
    "\n",
    "Full Prompt Construction: A prompt is constructed by combining:\n",
    " 1.A predefined system_instruction.\n",
    " 2.The retrieved existing_summary.\n",
    " 3.The current user_input.\n",
    "\n",
    "Response Generation: The LLM generates a response using the constructed prompt, which is then added back to the conversation memory (chat_memory).\n",
    "\n",
    "Output: The function returns the AI's response to be sent back to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_answer(vectorstore, user_id, user_input):\n",
    "    existing_summary = retrieve_user_summary(vectorstore, user_id)\n",
    "    convo_memory.chat_memory.add_user_message(user_input)\n",
    "    full_prompt = f\"{system_instruction}\\n{existing_summary}\\nUser: {user_input}\\n{convo_memory}\\nAI:\"\n",
    "    response = llm.invoke(full_prompt).content\n",
    "    summary_memory.chat_memory.add_user_message(user_input)\n",
    "    summary_memory.chat_memory.add_ai_message(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving the FAISS Index Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows saving the current FAISS index to a local file:\n",
    "\n",
    "Saving Process: The save_local() method is invoked with the specified file_path.\n",
    "\n",
    "Confirmation: A message confirms that the FAISS index has been saved.\n",
    "\n",
    "This is particularly useful for preserving the FAISS database between sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_Index(vectorstore, file_path = \"faiss_index\"):\n",
    "    vectorstore.save_local(file_path)\n",
    "    print(\"Chat saved\")\n",
    "    print(f\"FAISS index saved at: {file_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clearing the FAISS Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function resets the FAISS index to an empty state:\n",
    "\n",
    "Index Reset: A new empty FAISS index is created using FAISS.from_texts([\"\"], embedding=embeddings).\n",
    "\n",
    "Optional Save: The index can be saved after clearing (commented out in the code).\n",
    "\n",
    "Output: A message confirms that all data has been deleted.\n",
    "\n",
    "This is useful for cleaning up or restarting the FAISS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_faiss_index(file_path=\"faiss_index\"):\n",
    "    \"\"\"Clear the FAISS index by replacing it with an empty one.\"\"\"\n",
    "    global vectorstore\n",
    "    vectorstore = FAISS.from_texts([\"\"], embedding=embeddings)\n",
    "    # save_faiss_Index(vectorstore, file_path)\n",
    "    print(\"All data has been deleted from the FAISS database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing FAISS Index and Interactive User Input Loop:\n",
    "\n",
    "This code snippet demonstrates how to manage a FAISS index, handle user inputs in an interactive loop, and provide responses using a conversational AI system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the FAISS Index\n",
    "1. Purpose: \n",
    "Attempt to load a previously saved FAISS index from disk.\n",
    "\n",
    "2. Error Handling: \n",
    "If the FAISS index does not exist or cannot be loaded, an exception is caught, and a message indicates that a new FAISS index will be initialized.\n",
    "\n",
    "Outcome: Ensures the system always has a vectorstore instance, either loaded or newly created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing FAISS index found. Initializing a new one.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vectorstore = load_faiss_index(file_path=\"faiss_index\")\n",
    "    print(\"Loaded FAISS index from disk.\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing FAISS index found. Initializing a new one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. User Input and Interaction:\n",
    "\n",
    "1. User Identification: The user_id input identifies the specific user for whom the data is being processed and stored.\n",
    "\n",
    "2. Input Loop: A while loop continuously prompts the user to provide input until a specific condition (e.g., \"exit\" or \"delete data\") is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = input(\"Enter user_id :\")\n",
    "while True:\n",
    "    user_input = input(\"Say something: \")\n",
    "    if user_input == \"exit\":\n",
    "        store_final_summary(vectorstore, user_id)\n",
    "        break\n",
    "    if user_input == \"delete data\":\n",
    "        clear_faiss_index()\n",
    "        break\n",
    "    response = summarize_and_answer(vectorstore, user_id, user_input)\n",
    "    print(f\"AI: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-advanced-chatbot-8p_8ozoK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
